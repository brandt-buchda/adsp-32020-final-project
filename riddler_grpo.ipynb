{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install unsloth vllm\n",
    "!pip install --upgrade pillow\n",
    "!pip install llama-cpp-python\n",
    "!pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "import pandas as pd\n",
    "from unsloth import FastLanguageModel, PatchFastRL, is_bfloat16_supported\n",
    "from datasets import load_dataset, Dataset\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "PatchFastRL(\"GRPO\", FastLanguageModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n",
    "    max_seq_length = 512,\n",
    "    max_lora_rank = 64,\n",
    "    load_in_4bit = True,\n",
    "    fast_inference = True,\n",
    "    gpu_memory_utilization = 0.8)\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 64,\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha = 64,\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a dataset from Hugging Face (example: Riddles QA)\n",
    "df_riddles_qa = load_dataset(\"D4ve-R/riddles-qa\")\n",
    "\n",
    "# Save the train split (or any available split) to a CSV file\n",
    "df_riddles_qa[\"train\"].to_csv(\"data/riddles/riddles_qa.csv\")\n",
    "\n",
    "# Load a dataset from Hugging Face (example: Riddles QA)\n",
    "df_riddle_sense = load_dataset(\"INK-USC/riddle_sense\")\n",
    "\n",
    "# Function to map answerKey to actual answer text\n",
    "def get_correct_answer(example):\n",
    "    answer_index = example[\"choices\"][\"label\"].index(example[\"answerKey\"])  # Find index of correct answer\n",
    "    return example[\"choices\"][\"text\"][answer_index]  # Get corresponding answer text\n",
    "\n",
    "# Create DataFrame with riddle and correct answer\n",
    "df_riddle_sense = pd.DataFrame({\n",
    "    \"riddle\": df_riddle_sense[\"train\"][\"question\"],\n",
    "    \"answer\": [get_correct_answer(ex) for ex in df_riddle_sense[\"train\"]]\n",
    "})\n",
    "\n",
    "# Save the train split (or any available split) to a CSV file\n",
    "df_riddle_sense.to_csv(\"data/riddles/riddle_sense.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_riddles_qa = pd.read_csv(\"data/riddles/riddles_qa.csv\")\n",
    "df_riddle_sense = pd.read_csv(\"data/riddles/riddle_sense.csv\")\n",
    "df_riddles = pd.read_csv(\"data/riddles/Riddles.csv\", names=[\"riddle\", \"answer\", \"hint\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_riddles_qa.head())\n",
    "display(df_riddle_sense.head())\n",
    "display(df_riddles.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_riddles_qa = df_riddles_qa.drop_duplicates(subset=[\"answer\", \"riddle\"])\n",
    "# df_riddle_sense = df_riddle_sense.drop_duplicates(subset=[\"answer\", \"riddle\"])\n",
    "df_riddles = df_riddles.drop_duplicates(subset=[\"answer\", \"riddle\"])\n",
    "\n",
    "dataset = pd.concat([\n",
    "    df_riddles_qa.set_index([\"answer\", \"riddle\"]),\n",
    "    # df_riddle_sense.set_index([\"answer\", \"riddle\"]),\n",
    "    df_riddles.set_index([\"answer\", \"riddle\"])\n",
    "], axis=1).reset_index()\n",
    "\n",
    "dataset.drop(\"hint\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Long answer or reasoning should be filtered out\n",
    "dataset = dataset[dataset[\"answer\"].str.split().str.len() <= 4]\n",
    "dataset.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dataset)\n",
    "dataset = Dataset.from_pandas(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prep dataset\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "Solve the riddle and respond in the following format. Note the clue is an important keyword or phrase from the riddle:\n",
    "<clue>  \n",
    "...\n",
    "</clue>    \n",
    "<reason>  \n",
    "...\n",
    "</reason>  \n",
    "<answer>  \n",
    "...\n",
    "</answer>  \n",
    "\"\"\"\n",
    "\n",
    "def extract_xml_tag(text: str, tag: str) -> str:\n",
    "    answer = text.split(f\"<{tag}>\")[-1]\n",
    "    answer = answer.split(f\"</{tag}>\")[0]\n",
    "    return answer.strip()\n",
    "\n",
    "# uncomment middle messages for 1-shot prompting\n",
    "def get_riddles() -> Dataset:\n",
    "    # Load CSVs\n",
    "    df_riddles_qa = pd.read_csv(\"data/riddles/riddles_qa.csv\")\n",
    "    #df_riddle_sense = pd.read_csv(\"data/riddles/riddle_sense.csv\")\n",
    "    df_riddles = pd.read_csv(\"data/riddles/Riddles.csv\", names=[\"riddle\", \"answer\", \"hint\"])\n",
    "\n",
    "    # Drop duplicates\n",
    "    for df in [df_riddles_qa, df_riddle_sense, df_riddles]:\n",
    "        df.drop_duplicates(subset=[\"answer\", \"riddle\"], inplace=True)\n",
    "\n",
    "    # Merge data\n",
    "    dataset = pd.concat([\n",
    "        df_riddles_qa.set_index([\"answer\", \"riddle\"]),\n",
    "        #df_riddle_sense.set_index([\"answer\", \"riddle\"]),\n",
    "        df_riddles.set_index([\"answer\", \"riddle\"])\n",
    "    ], axis=1).reset_index()\n",
    "\n",
    "    # Drop hint column safely\n",
    "    dataset.drop(columns=[\"hint\"], errors=\"ignore\", inplace=True)\n",
    "\n",
    "    # Keep only short answers (â‰¤ 4 words)\n",
    "    dataset = dataset[dataset[\"answer\"].str.split().str.len() <= 4]\n",
    "\n",
    "    # Drop missing or empty answers\n",
    "    dataset = dataset.dropna(subset=[\"answer\"])\n",
    "    dataset = dataset[dataset[\"answer\"].str.strip() != \"\"]\n",
    "\n",
    "    # Convert to structured format\n",
    "    records = dataset.apply(lambda row: {\n",
    "        'prompt': [\n",
    "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "            {'role': 'user', 'content': row['riddle']}\n",
    "        ],\n",
    "        'answer': row['answer']\n",
    "    }, axis=1).to_list()\n",
    "\n",
    "    return Dataset.from_list(records)\n",
    "\n",
    "dataset=get_riddles()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Reward Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Correct Answer\n",
    "Rewards if the answer and preferred answer are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
    "    responses = [completion[0]['content'].strip().lower() for completion in completions]\n",
    "    extracted_responses = [extract_xml_tag(r, \"answer\").strip().lower() for r in responses]\n",
    "    correct_answers = [a.strip().lower() for a in answer]\n",
    "    q = prompts[0][-1]['content']\n",
    "\n",
    "    def is_similar(r, a):\n",
    "        return r in a or a in r\n",
    "\n",
    "    print('-'*20, f\"\\nRiddle:\\n{q}\", f\"\\nAnswer:\\n{answer[0]}\\n\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\\n\")\n",
    "\n",
    "    return [2.0 if is_similar(r, a) else 0.0 for r, a in zip(extracted_responses, correct_answers)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Strict Correct XML Format\n",
    "Rewards a perfectly formatted response with proper XML tag order and structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strict_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    pattern = r\"^<clue>\\n.*?\\n</clue>\\n<reason>\\n.*?\\n</reason>\\n<answer>\\n.*?\\n</answer>$\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, r) and r.endswith(\"</answer>\") and r.startswith(\"<clue>\") for r in responses]\n",
    "    return [0.5 if match else 0.0 for match in matches]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Soft Correct XML Format\n",
    "Rewards a response as long as the XML is parsable. Does not whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    pattern = r\"^<clue>\\s*.*?\\s*</clue>\\s*<reason>\\s*.*?\\s*</reason>\\s*<answer>\\s*.*?\\s*</answer>$\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, r) for r in responses]\n",
    "    return [0.5 if match else 0.0 for match in matches]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### XML Count\n",
    "Rewards if the correct number of XML tags are present in any order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_xml(text) -> float:\n",
    "    count = 0.0\n",
    "    if text.count(\"<clue>\\n\") == 1:\n",
    "        count += 0.125\n",
    "    if text.count(\"\\n</clue>\\n\") == 1:\n",
    "        count += 0.125\n",
    "    if text.count(\"<reason>\\n\") == 1:\n",
    "        count += 0.125\n",
    "    if text.count(\"\\n</reason>\\n\") == 1:\n",
    "        count += 0.125\n",
    "    if text.count(\"\\n<answer>\\n\") == 1:\n",
    "        count += 0.125\n",
    "    if text.count(\"\\n</answer>\") == 1:\n",
    "        count += 0.125\n",
    "    return count\n",
    "\n",
    "def xmlcount_reward_func(completions, **kwargs) -> list[float]:\n",
    "    contents = [completion[0][\"content\"] for completion in completions]\n",
    "    return [count_xml(c) for c in contents]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Answer Brevity\n",
    "Rewards short answers, less than 4 words. All answers in the dataset are 4 words or less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def short_reward_func(completions, **kwargs) -> list[float]:\n",
    "    responses = [completion[0]['content'].strip() for completion in completions]\n",
    "    extracted_responses = [extract_xml_tag(r, \"answer\").strip() for r in responses]\n",
    "    \n",
    "    def shortness_score(r):\n",
    "        words = r.split()\n",
    "        if len(words) <= 4:\n",
    "            return 0.5\n",
    "        return 0.0\n",
    "    \n",
    "    return [shortness_score(r) for r in extracted_responses]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Clue Extraction\n",
    "Rewards a clue that is a substring from the original riddle. (Must be robust against model responding with individual letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reward(clue, riddle):\n",
    "    clue_length = len(clue.split())\n",
    "    riddle_words = set(riddle.split())\n",
    "\n",
    "    if clue_length < 1 or clue_length > 5:\n",
    "        return 0.0  \n",
    "\n",
    "    return 0.5 if clue in riddle else 0.0\n",
    "\n",
    "\n",
    "def clue_extraction_reward_func(prompts, completions, **kwargs) -> list[float]:\n",
    "    responses = [completion[0]['content'].strip() for completion in completions]\n",
    "    extracted_clues = [extract_xml_tag(r, \"clue\").strip() for r in responses]\n",
    "    riddle = prompts[0][-1]['content']\n",
    "    \n",
    "    return [compute_reward(clue, riddle) for clue in extracted_clues]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# Initialize WandB\n",
    "wandb.init(project=\"riddle-llm-training\", name=\"grpo_run_5\", config={\n",
    "    \"learning_rate\": 5e-6,\n",
    "    \"num_train_epochs\": 10,\n",
    "    \"batch_size\": 1,\n",
    "    \"max_steps\": 50000,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import GRPOConfig, GRPOTrainer\n",
    "training_args = GRPOConfig(\n",
    "    use_vllm = True, # use vLLM for fast inference!\n",
    "    learning_rate = 5e-6,\n",
    "    adam_beta1 = 0.9,\n",
    "    adam_beta2 = 0.99,\n",
    "    weight_decay = 0.1,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    optim = \"paged_adamw_8bit\",\n",
    "    logging_steps = 1,\n",
    "    bf16 = is_bfloat16_supported(),\n",
    "    fp16 = not is_bfloat16_supported(),\n",
    "    per_device_train_batch_size = 1,\n",
    "    gradient_accumulation_steps = 1, # Increase to 4 for smoother training\n",
    "    num_generations = 8, # Decrease if out of memory\n",
    "    max_prompt_length = 256,\n",
    "    max_completion_length = 200,\n",
    "    num_train_epochs = 10, # Set to 1 for a full training run\n",
    "    max_steps = 10000,\n",
    "    save_steps = 250,\n",
    "    max_grad_norm = 0.1,\n",
    "    report_to = \"wandb\", # Can use Weights & Biases\n",
    "    output_dir = \"outputs\",\n",
    ")\n",
    "\n",
    "# Log training arguments to WandB\n",
    "wandb.config.update(vars(training_args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = GRPOTrainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,\n",
    "    reward_funcs = [\n",
    "        xmlcount_reward_func,\n",
    "        soft_format_reward_func,\n",
    "        strict_format_reward_func,\n",
    "        short_reward_func,\n",
    "        clue_extraction_reward_func,\n",
    "        correctness_reward_func,\n",
    "    ],\n",
    "    args = training_args,\n",
    "    train_dataset = dataset,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m128",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m128"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
